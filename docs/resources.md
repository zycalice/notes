---
title: resources
permalink: /resources/
---

# Resources (Mostly course resources)
[<< Back to my website](https://zycalice.github.io/)

## Linear Algebra
- Matrix Calculus: [https://atmos.washington.edu/~dennis/MatrixCalculus.pdf](https://atmos.washington.edu/~dennis/MatrixCalculus.pdf)

## Statistics
- CTT: [https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/BS704_Probability12.html](https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_probability/BS704_Probability12.html)
- CLT: [https://www.me.psu.edu/cimbala/me345/Lectures/Central_Limit_Theorem.pdf](https://www.me.psu.edu/cimbala/me345/Lectures/Central_Limit_Theorem.pdf)
- Regression to mean and how to remove it: [https://www.iwh.on.ca/what-researchers-mean-by/regression-to-mean](https://www.iwh.on.ca/what-researchers-mean-by/regression-to-mean)

## Information Theory
- [https://web.stanford.edu/class/ee376a/files/lecture_notes.pdf](https://web.stanford.edu/class/ee376a/files/lecture_notes.pdf)
- Uncorrelated vs independent: [https://www.stat.cmu.edu/~cshalizi/uADA/13/reminders/uncorrelated-vs-independent.pdf](https://www.stat.cmu.edu/~cshalizi/uADA/13/reminders/uncorrelated-vs-independent.pdf)
- Huffman coding: [http://www.cs.cmu.edu/~aarti/Class/10704/lec8-srccodingHuffman.pdf](http://www.cs.cmu.edu/~aarti/Class/10704/lec8-srccodingHuffman.pdf)

## Learning Theory, PAC Learning and VC dimention
- PAC: [https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture28-pac.pdf](https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture28-pac.pdf)
- PAC: [https://www.cs.princeton.edu/courses/archive/spring15/cos511/notes/lec3.pdf](https://www.cs.princeton.edu/courses/archive/spring15/cos511/notes/lec3.pdf)
- Learning Theory: [https://courses.cs.washington.edu/courses/cse546/12wi/slides/cse546wi12LearningTheory.pdf](https://courses.cs.washington.edu/courses/cse546/12wi/slides/cse546wi12LearningTheory.pdf)

## Bayes Net
- Bayes Belief Nets: [https://plato.stanford.edu/entries/artificial-intelligence/bayesian-nets.html](https://plato.stanford.edu/entries/artificial-intelligence/bayesian-nets.html)
- Naive Bayes: [http://www.cs.cmu.edu/~10601b/slides/NBayes.pdf](http://www.cs.cmu.edu/~10601b/slides/NBayes.pdf)

## Hidden Markov Model
- HMM: [http://www.cs.cmu.edu/~tbergkir/11711fa17/recitation4_notes.pdf](http://www.cs.cmu.edu/~tbergkir/11711fa17/recitation4_notes.pdf)

## Recurrent Neural Network
- RNN cheatsheet: [https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)
- HMM vs RNN: [https://arxiv.org/pdf/1907.04670.pdf](https://arxiv.org/pdf/1907.04670.pdf)

## Linear Regression and Logistic Regression
- Pearson and Spearman Correlation Comparison: [https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/](https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/a-comparison-of-the-pearson-and-spearman-correlation-methods/)
- Logistic regression: [https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/)

## Kernel
- Berkeley: [https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c02_p25-46.pdf](https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c02_p25-46.pdf)

## Online Learning
- MIT: [https://www.mit.edu/~9.520/spring11/slides/class15_online.pdf](https://www.mit.edu/~9.520/spring11/slides/class15_online.pdf)
- [http://www.ciml.info/dl/v0_8/ciml-v0_8-ch03.pdf](http://www.ciml.info/dl/v0_8/ciml-v0_8-ch03.pdf)

## Support Vector Machine
- [https://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf](https://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf)
- Berkeley: [https://www.eecs189.org/static/notes/n20.pdf](https://www.eecs189.org/static/notes/n20.pdf)
- CMU: [http://www.cs.cmu.edu/~aarti/Class/10701_Spring14/slides/SupportVectorMachines.pdf](http://www.cs.cmu.edu/~aarti/Class/10701_Spring14/slides/SupportVectorMachines.pdf)
- Toronto: [http://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/SupportVectorMachines.pdf](http://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/SupportVectorMachines.pdf)
- Cornell: [https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html)

## Principal Component Analysis and Auto-encoders
- Covariance Matrix Definition: [https://www.itl.nist.gov/div898/handbook/pmc/section5/pmc541.htm](https://www.itl.nist.gov/div898/handbook/pmc/section5/pmc541.htm)
- SVD: [http://web.mit.edu/course/other/be.400/OldFiles/www/SVD/Singular_Value_Decomposition.htm](http://web.mit.edu/course/other/be.400/OldFiles/www/SVD/Singular_Value_Decomposition.htm)
- PCA derivation: [https://www.eecs189.org/static/notes/n10.pdf](https://www.eecs189.org/static/notes/n10.pdf)
- DeepLearningBook: [https://www.deeplearningbook.org/contents/autoencoders.html](https://www.deeplearningbook.org/contents/autoencoders.html)
- VAE: [https://arxiv.org/pdf/1606.05908.pdf](https://arxiv.org/pdf/1606.05908.pdf)
- Auto-encoder using label information: [https://jmlr.csail.mit.edu/papers/volume13/snoek12a/snoek12a.pdf](https://jmlr.csail.mit.edu/papers/volume13/snoek12a/snoek12a.pdf)

## Optimization
- Lagrangian Duality: [https://www-cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf](https://www-cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf)
### Gradient Descent
- UBC: [https://www.cs.ubc.ca/~schmidtm/Courses/540-W18/L4.pdf](https://www.cs.ubc.ca/~schmidtm/Courses/540-W18/L4.pdf)
- Berkeley: [https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c03_p47-84.pdf](https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c03_p47-84.pdf)

## Deep Learning
- Bandit algorithm: [https://tor-lattimore.com/downloads/book/book.pdf](https://tor-lattimore.com/downloads/book/book.pdf)
- Visual Question Answering: [https://visualqa.org/](https://visualqa.org/)

## Cross Validation
- Error and validation: [http://www.stat.cmu.edu/~ryantibs/advmethods/notes/errval.pdf](http://www.stat.cmu.edu/~ryantibs/advmethods/notes/errval.pdf)
- K-Fold cross validation: [http://statweb.stanford.edu/~tibs/sta306bfiles/cvwrong.pdf](http://statweb.stanford.edu/~tibs/sta306bfiles/cvwrong.pdf)
- DataCamp practice: [https://campus.datacamp.com/courses/model-validation-in-python/cross-validation?ex=6](https://campus.datacamp.com/courses/model-validation-in-python/cross-validation?ex=6)

## Evaluation
- Precision and recall: [https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)
- Precision and recall: [https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)
- Precision, recall, sensitivity and specificity: [https://deepai.org/machine-learning-glossary-and-terms/precision-and-recall](https://deepai.org/machine-learning-glossary-and-terms/precision-and-recall)
- Precision, recall, sensitivity and specificity: [http://cs229.stanford.edu/section/evaluation_metrics_spring2020.pdf](http://cs229.stanford.edu/section/evaluation_metrics_spring2020.pdf)
- [https://scholar.harvard.edu/files/msseo/files/5.convergence_informatics_week5.pdf](https://scholar.harvard.edu/files/msseo/files/5.convergence_informatics_week5.pdf)

## NLP
### Language models, Topic Modeling
- N-gram: [https://web.stanford.edu/~jurafsky/slp3/3.pdf](https://web.stanford.edu/~jurafsky/slp3/3.pdf)
- Eigenwords: [https://jmlr.csail.mit.edu/papers/volume16/dhillon15a/dhillon15a.pdf](https://jmlr.csail.mit.edu/papers/volume16/dhillon15a/dhillon15a.pdf)
- Latent Dirtchlet Allocation/Topic Modeling: [https://www.cl.cam.ac.uk/teaching/1213/L101/clark_lectures/lect7.pdf](https://www.cl.cam.ac.uk/teaching/1213/L101/clark_lectures/lect7.pdf)
### Encoder Decoder
- Encoder and Decoder with Attention: [https://bastings.github.io/annotated_encoder_decoder/](https://bastings.github.io/annotated_encoder_decoder/)
- Decoding methods: [https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc](https://towardsdatascience.com/decoding-strategies-that-you-need-to-know-for-response-generation-ba95ee0faadc)
- Transformer - Implementation of paper "Attention is All You Need: [http://nlp.seas.harvard.edu/2018/04/03/attention.html](http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding)

### NLP Applications:
- NLP and argument: [https://www.aclweb.org/anthology/P16-5002/](https://www.aclweb.org/anthology/P16-5002/)
- NLP and persuasion: [https://nlds.soe.ucsc.edu/persuasion_persona](https://nlds.soe.ucsc.edu/persuasion_persona)
- Seq to Seq and Translation for NLP: [https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)

## Model Explanations and Interpretable ML
- LIME: [https://homes.cs.washington.edu/~marcotcr/blog/lime/](https://homes.cs.washington.edu/~marcotcr/blog/lime/)
- LIME Paper: [https://arxiv.org/pdf/1602.04938v1.pdf](https://arxiv.org/pdf/1602.04938v1.pdf)
- LIME GitHub: [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)
- LIME Tutorial: [https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html](https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html)
- Interpretable ML: [https://christophm.github.io/interpretable-ml-book/limo.html](https://christophm.github.io/interpretable-ml-book/limo.html)

## Tools
- GitPages: [https://docs.github.com/en/free-pro-team@latest/github/working-with-github-pages](https://docs.github.com/en/free-pro-team@latest/github/working-with-github-pages)
- Jekyllrb: [https://jekyllrb.com/docs/github-pages/](https://jekyllrb.com/docs/github-pages/)
- Front Matter: [https://jekyllrb.com/docs/front-matter/](https://jekyllrb.com/docs/front-matter/)
- JSON resume: [https://jsonresume.org/getting-started/](https://jsonresume.org/getting-started/)

## Pytorch
- Pytorch Intro Examples: [https://pytorch.org/tutorials/beginner/pytorch_with_examples.html](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)
- Pytorch Access Weights: [https://discuss.pytorch.org/t/access-weights-of-a-specific-module-in-nn-sequential/3627](https://discuss.pytorch.org/t/access-weights-of-a-specific-module-in-nn-sequential/3627)
- Pytorch Latent Space: [https://discuss.pytorch.org/t/plotting-the-latent-space/88431](https://discuss.pytorch.org/t/plotting-the-latent-space/88431)

## Datasets
- Lionbridge: [https://lionbridge.ai/datasets/](https://lionbridge.ai/datasets/)

## Other Blog posts
- Autoencoder: [https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798#28b1](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798#28b1)
- Cosine Similarity vs Euclidean Distance: [https://cmry.github.io/notes/euclidean-v-cosine](https://cmry.github.io/notes/euclidean-v-cosine)
- Naive Bayes: [https://towardsdatascience.com/all-about-naive-bayes-8e13cef044cf](https://towardsdatascience.com/all-about-naive-bayes-8e13cef044cf)
